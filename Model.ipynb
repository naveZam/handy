{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22114eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e6a3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "def read_from_folder(directory, input_shape):\n",
    "    input = np.empty(input_shape, dtype='uint8')\n",
    "    image_count = 0\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        if os.path.isfile(f) and (f.endswith('.jpg') or f.endswith('.jpeg')):\n",
    "            img = Image.open(f)\n",
    "    \n",
    "            # Resize\n",
    "            size = 176, 100\n",
    "            img = img.resize(size, Image.LANCZOS)\n",
    "    \n",
    "            numpydata = np.array(img)\n",
    "            input[image_count] = numpydata\n",
    "    \n",
    "            image_count += 1\n",
    "    \n",
    "            if image_count == 37:\n",
    "                break  # Stop processing if 37 images are reached\n",
    "    \n",
    "    while image_count < 37:\n",
    "        input[image_count] = input[image_count - 1]\n",
    "        image_count += 1\n",
    "    \n",
    "    return input\n",
    "\n",
    "def get_csv_dict(directory, labels): ### IMPORTANT TO REMEMBER: This is for the limited csv, you MUST change this for the full version.\n",
    "    data = pd.read_csv(directory)\n",
    "    data_dict = {i: [] for i in labels}\n",
    "    desired_rows = 1500\n",
    "    \n",
    "    for label in labels:\n",
    "        filtered_data = data[data['label_id'].isin([label])] #### TODO: Make everything 1500 length\n",
    "\n",
    "        if len(filtered_data) >= desired_rows:\n",
    "            data_dict[label] =  filtered_data['video_id'].values[:desired_rows]\n",
    "            continue\n",
    "    \n",
    "        rows_to_add = desired_rows - len(filtered_data)\n",
    "        random_rows = filtered_data.sample(n=rows_to_add, replace=True)\n",
    "        data_dict[label] = pd.concat([filtered_data, random_rows], ignore_index=True)['video_id'].values\n",
    "\n",
    "    return data_dict\n",
    "        \n",
    "def generate_data_labels(labels, label, length):\n",
    "    data_label = []\n",
    "    for i in range(length):\n",
    "        result = [1 if labels[j] == label else 0 for j in range(len(labels))]\n",
    "        data_label.append(result)\n",
    "    return np.array(data_label)\n",
    "\n",
    "def get_batch(directory, data_dict, chosen_labels, label, amount, index):    \n",
    "    data = np.array([])\n",
    "\n",
    "    for i in range(index, (index+amount)):\n",
    "        example = read_from_folder(f\"{directory}/{data_dict[label][i]}\", (37, 100, 176, 3))\n",
    "        example = np.expand_dims(example, axis=0)\n",
    "        data = append_or_copy_array(data, example)\n",
    "    \n",
    "    #data = read_from_file(data_directory + \"data\" + \"_batch_\" + str(batch_num) + \".npy\")\n",
    "    data_labels = generate_data_labels(chosen_labels, label, len(data))\n",
    "    \n",
    "    index += amount\n",
    "    return data, data_labels, index\n",
    "\n",
    "def append_or_copy_array(base_array, new_array):\n",
    "    if base_array.size == 0:\n",
    "        result_array = new_array\n",
    "    else:\n",
    "        result_array = np.concatenate((base_array, new_array), axis=0)\n",
    "\n",
    "    return result_array\n",
    "\n",
    "def load_model_names(log_file):\n",
    "    model_names = set()\n",
    "\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, 'r') as file:\n",
    "            model_names = set(file.read().splitlines())\n",
    "\n",
    "    return model_names\n",
    "\n",
    "def cleanup_models(save_dir, log_file, model_names):\n",
    "    while len(model_names) > 10:\n",
    "        oldest_model = min(model_names)\n",
    "        oldest_model_path = os.path.join(save_dir, oldest_model)\n",
    "\n",
    "        # Delete the oldest model file\n",
    "        if os.path.exists(oldest_model_path):\n",
    "            os.remove(oldest_model_path)\n",
    "\n",
    "        # Remove the oldest model name from the set and the log file\n",
    "        model_names.remove(oldest_model)\n",
    "        with open(log_file, 'w') as file:\n",
    "            file.write('\\n'.join(model_names) + '\\n')\n",
    "\n",
    "def save_model(model, save_dir, log_file, model_name_prefix='model'):\n",
    "    # Generate a unique model name\n",
    "    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    model_name = f'{model_name_prefix}_{timestamp}.keras'\n",
    "\n",
    "    # Save the model\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    model.save(model_path)\n",
    "\n",
    "    # Update the model names set\n",
    "    model_names = load_model_names(log_file)\n",
    "    model_names.add(model_name)\n",
    "\n",
    "    # Update the log file with the new model name\n",
    "    with open(log_file, 'a') as file:\n",
    "        file.write(model_name + '\\n')\n",
    "\n",
    "    # Remove oldest model if exceeding the maximum allowed models\n",
    "    cleanup_models(save_dir, log_file, model_names)\n",
    "\n",
    "    print(f'Model saved: {model_name}')\n",
    "\n",
    "def load_model(model_path):\n",
    "    return tf.keras.models.load_model(model_path)\n",
    "\n",
    "def load_model_from_dir(save_dir, model):\n",
    "    saved_models = [f for f in os.listdir(save_dir) if f.startswith('model_')]\n",
    "\n",
    "    if not saved_models:\n",
    "        print('No saved models found.')\n",
    "        return model\n",
    "\n",
    "    print('Saved Models:')\n",
    "    for i, model_name in enumerate(saved_models, start=1):\n",
    "        print(f'{i}. {model_name}')\n",
    "\n",
    "    choice = input('Do you want to load a model? (y/n): ').lower()\n",
    "\n",
    "    if choice == 'y':\n",
    "        model_choice = int(input('Enter the number of the model to load: '))\n",
    "        if 1 <= model_choice <= len(saved_models):\n",
    "            selected_model = saved_models[model_choice - 1]\n",
    "            model_path = os.path.join(save_dir, selected_model)\n",
    "            loaded_model = load_model(model_path)\n",
    "            print(f'Model {selected_model} loaded successfully.')\n",
    "            return loaded_model\n",
    "        else:\n",
    "            print('Invalid choice. No model loaded.')\n",
    "            return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', input_shape=(37, 100, 176, 3)),\n",
    "    tf.keras.layers.MaxPooling3D((2, 2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Conv3D(64, (3, 3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling3D((2, 2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Conv3D(128, (3, 3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling3D((2, 2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82ca8075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/navezamb/project/Train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Compiling the model\u001b[39;00m\n\u001b[0;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mloss_fn, optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n\u001b[1;32m---> 23\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m \u001b[43mget_csv_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchosen_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m val_dict \u001b[38;5;241m=\u001b[39m get_csv_dict(val_csv, chosen_labels)\n\u001b[0;32m     26\u001b[0m load_model_from_dir(model_saves, model)\n",
      "Cell \u001b[1;32mIn[8], line 31\u001b[0m, in \u001b[0;36mget_csv_dict\u001b[1;34m(directory, labels)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_csv_dict\u001b[39m(directory, labels): \u001b[38;5;66;03m### IMPORTANT TO REMEMBER: This is for the limited csv, you MUST change this for the full version.\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     data_dict \u001b[38;5;241m=\u001b[39m {i: [] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m labels}\n\u001b[0;32m     33\u001b[0m     desired_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1500\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/navezamb/project/Train.csv'"
     ]
    }
   ],
   "source": [
    "## Traning on batch\n",
    "\n",
    "data_directory = \"/mnt/ori/project/data/20bn-jester-v1\"\n",
    "data_csv = \"/home/navezamb/project/Train.csv\"\n",
    "val_csv = \"/home/navezamb/project/Validation.csv\"\n",
    "chosen_labels = [14, 15, 16, 17, 18]\n",
    "log_file = \"model_log.txt\"\n",
    "log_accuracy_file = \"model_accuracy_log.txt\"\n",
    "model_saves = \"/mnt/ori/project/model-checkpoints\"\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = 'categorical_crossentropy'\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Training loop using train_on_batch\n",
    "num_epochs = 10\n",
    "batch_size = 30  # Adjust batch size as needed\n",
    "print(\"starting...\")\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "\n",
    "data_dict = get_csv_dict(data_csv, chosen_labels)\n",
    "val_dict = get_csv_dict(val_csv, chosen_labels)\n",
    "\n",
    "load_model_from_dir(model_saves, model)\n",
    "\n",
    "accuracy_list = np.array([])\n",
    "val_accuracy_list = np.array([])\n",
    "\n",
    "for batch in range(int(len(data_dict[chosen_labels[0]]) / batch_size)):\n",
    "    data = np.array([])\n",
    "    data_labels = np.array([])\n",
    "    test = np.array([])\n",
    "    test_labels = np.array([])\n",
    "\n",
    "    data_index = 0\n",
    "    val_index = 0\n",
    "    \n",
    "    for label in chosen_labels:\n",
    "        data_batch, data_labels_batch, data_index = get_batch(data_directory, data_dict, chosen_labels, label, batch_size, data_index)\n",
    "        test_batch, test_labels_batch, val_inedx = get_batch(data_directory, val_dict, chosen_labels, label, int(batch_size / 10), val_index)\n",
    "        \n",
    "        data = append_or_copy_array(data, data_batch)\n",
    "        data_labels = append_or_copy_array(data_labels, data_labels_batch)\n",
    "        test = append_or_copy_array(test, test_batch)\n",
    "        test_labels = append_or_copy_array(test_labels, test_labels_batch)\n",
    "        \n",
    "    data = tf.convert_to_tensor(data)\n",
    "    data_labels = tf.convert_to_tensor(data_labels)\n",
    "    test = tf.convert_to_tensor(test)\n",
    "    test_labels = tf.convert_to_tensor(test_labels)\n",
    "    \n",
    "    print(data.shape, data_labels.shape, test.shape, test_labels.shape)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            print(i)\n",
    "            batch_data = data[i:i+batch_size]\n",
    "            batch_labels = data_labels[i:i+batch_size]\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(batch_data)\n",
    "                if loss_fn == 'categorical_crossentropy':\n",
    "                    loss = tf.keras.losses.categorical_crossentropy(batch_labels, predictions)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid loss function specified.\")\n",
    "\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            total_loss += loss.numpy() \n",
    "            correct_predictions += np.sum(np.argmax(predictions, axis=1) == np.argmax(batch_labels, axis=1))\n",
    "\n",
    "        average_loss = total_loss / (len(data) / batch_size)\n",
    "        accuracy = correct_predictions / len(data)\n",
    "\n",
    "        # Validation\n",
    "        val_predictions = model(test)\n",
    "        val_loss = tf.keras.losses.categorical_crossentropy(test_labels, val_predictions).numpy()\n",
    "        val_accuracy = np.sum(np.argmax(val_predictions, axis=1) == np.argmax(test_labels, axis=1)) / len(test)\n",
    "\n",
    "        save_model(model, model_saves, log_file)\n",
    "\n",
    "        accuracy_list = np.append(accuracy_list, accuracy)\n",
    "        val_accuracy_list = np.append(val_accuracy_list, val_accuracy)\n",
    "\n",
    "        # Update the log file with the new model name\n",
    "        with open(log_accuracy_file, 'a') as file:\n",
    "            file.write(f'Epoch {epoch+1}, Loss: {average_loss}, Accuracy: {accuracy}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}' + '\\n\\n')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {average_loss}, Accuracy: {accuracy}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0499ebaa-c98e-45b0-b1d4-a72cf0950c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the model \n",
    "model.save('Model_test.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada3c38-6dee-4ee0-b643-b2f63a388ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph the accuracy\n",
    "# Plot the data\n",
    "plt.plot(accuracy_list)\n",
    "plt.plot(val_accuracy_list)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Epoch')\n",
    "plt.title('Modle Accuracy Plot')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
