{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22114eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e6a3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "def read_from_folder(directory, input_shape):\n",
    "    input = np.empty(input_shape, dtype='uint8')\n",
    "    image_count = 0\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        if os.path.isfile(f) and (f.endswith('.jpg') or f.endswith('.jpeg')):\n",
    "            img = Image.open(f)\n",
    "    \n",
    "            # Resize\n",
    "            size = 176, 100\n",
    "            img = img.resize(size, Image.LANCZOS)\n",
    "    \n",
    "            numpydata = np.array(img)\n",
    "            input[image_count] = numpydata\n",
    "    \n",
    "            image_count += 1\n",
    "    \n",
    "            if image_count == 37:\n",
    "                break  # Stop processing if 37 images are reached\n",
    "    \n",
    "    while image_count < 37:\n",
    "        input[image_count] = input[image_count - 1]\n",
    "        image_count += 1\n",
    "    \n",
    "    return input\n",
    "\n",
    "def get_csv_dict(directory, labels): ### IMPORTANT TO REMEMBER: This is for the limited csv, you MUST change this for the full version.\n",
    "    data = pd.read_csv(directory)\n",
    "    data_dict = {i: [] for i in labels}\n",
    "    desired_rows = 1500\n",
    "    \n",
    "    for label in labels:\n",
    "        filtered_data = data[data['label_id'].isin([label])] #### TODO: Make everything 1500 length\n",
    "\n",
    "        if len(filtered_data) >= desired_rows:\n",
    "            data_dict[label] =  filtered_data['video_id'].values[:desired_rows]\n",
    "            continue\n",
    "    \n",
    "        rows_to_add = desired_rows - len(filtered_data)\n",
    "        random_rows = filtered_data.sample(n=rows_to_add, replace=True)\n",
    "        data_dict[label] = pd.concat([filtered_data, random_rows], ignore_index=True)['video_id'].values\n",
    "\n",
    "    return data_dict\n",
    "        \n",
    "def generate_data_labels(labels, label, length):\n",
    "    data_label = []\n",
    "    for i in range(length):\n",
    "        result = [1 if labels[j] == label else 0 for j in range(len(labels))]\n",
    "        data_label.append(result)\n",
    "    return np.array(data_label)\n",
    "\n",
    "def get_batch(directory, data_dict, chosen_labels, label, amount, index):    \n",
    "    data = np.array([])\n",
    "\n",
    "    for i in range(index, (index+amount)):\n",
    "        example = read_from_folder(f\"{directory}/{data_dict[label][i]}\", (37, 100, 176, 3))\n",
    "        example = np.expand_dims(example, axis=0)\n",
    "        data = append_or_copy_array(data, example)\n",
    "    \n",
    "    #data = read_from_file(data_directory + \"data\" + \"_batch_\" + str(batch_num) + \".npy\")\n",
    "    data_labels = generate_data_labels(chosen_labels, label, len(data))\n",
    "    \n",
    "    index += amount\n",
    "    return data, data_labels, index\n",
    "\n",
    "def append_or_copy_array(base_array, new_array):\n",
    "    if base_array.size == 0:\n",
    "        result_array = new_array\n",
    "    else:\n",
    "        result_array = np.concatenate((base_array, new_array), axis=0)\n",
    "\n",
    "    return result_array\n",
    "\n",
    "def load_model_names(log_file):\n",
    "    model_names = set()\n",
    "\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, 'r') as file:\n",
    "            model_names = set(file.read().splitlines())\n",
    "\n",
    "    return model_names\n",
    "\n",
    "def cleanup_models(save_dir, log_file, model_names):\n",
    "    while len(model_names) > 10:\n",
    "        oldest_model = min(model_names)\n",
    "        oldest_model_path = os.path.join(save_dir, oldest_model)\n",
    "\n",
    "        # Delete the oldest model file\n",
    "        if os.path.exists(oldest_model_path):\n",
    "            os.remove(oldest_model_path)\n",
    "\n",
    "        # Remove the oldest model name from the set and the log file\n",
    "        model_names.remove(oldest_model)\n",
    "        with open(log_file, 'w') as file:\n",
    "            file.write('\\n'.join(model_names) + '\\n')\n",
    "\n",
    "def save_model(model, save_dir, log_file, model_name_prefix='model'):\n",
    "    # Generate a unique model name\n",
    "    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    model_name = f'{model_name_prefix}_{timestamp}.keras'\n",
    "\n",
    "    # Save the model\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    model.save(model_path)\n",
    "\n",
    "    # Update the model names set\n",
    "    model_names = load_model_names(log_file)\n",
    "    model_names.add(model_name)\n",
    "\n",
    "    # Update the log file with the new model name\n",
    "    with open(log_file, 'a') as file:\n",
    "        file.write(model_name + '\\n')\n",
    "\n",
    "    # Remove oldest model if exceeding the maximum allowed models\n",
    "    cleanup_models(save_dir, log_file, model_names)\n",
    "\n",
    "    print(f'Model saved: {model_name}')\n",
    "\n",
    "def load_model(model_path):\n",
    "    return tf.keras.models.load_model(model_path)\n",
    "\n",
    "def load_model_from_dir(save_dir, model):\n",
    "    saved_models = [f for f in os.listdir(save_dir) if f.startswith('model_')]\n",
    "\n",
    "    if not saved_models:\n",
    "        print('No saved models found.')\n",
    "        return model\n",
    "\n",
    "    print('Saved Models:')\n",
    "    for i, model_name in enumerate(saved_models, start=1):\n",
    "        print(f'{i}. {model_name}')\n",
    "\n",
    "    choice = input('Do you want to load a model? (y/n): ').lower()\n",
    "\n",
    "    if choice == 'y':\n",
    "        model_choice = int(input('Enter the number of the model to load: '))\n",
    "        if 1 <= model_choice <= len(saved_models):\n",
    "            selected_model = saved_models[model_choice - 1]\n",
    "            model_path = os.path.join(save_dir, selected_model)\n",
    "            loaded_model = load_model(model_path)\n",
    "            print(f'Model {selected_model} loaded successfully.')\n",
    "            return loaded_model\n",
    "        else:\n",
    "            print('Invalid choice. No model loaded.')\n",
    "            return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14bc9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model architecture\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', input_shape=(37, 100, 176, 3)),\n",
    "    tf.keras.layers.MaxPooling3D((2, 2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),  # Add dropout with a rate of 25%\n",
    "    tf.keras.layers.Conv3D(64, (3, 3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling3D((2, 2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),  # Add another dropout layer with the same rate\n",
    "    tf.keras.layers.Conv3D(64, (3, 3, 3), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "82ca8075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n",
      "Saved Models:\n",
      "1. model_20231228140503.keras\n",
      "2. model_20231228140530.keras\n",
      "3. model_20231228140618.keras\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to load a model? (y/n):  y\n",
      "Enter the number of the model to load:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model model_20231228140503.keras loaded successfully.\n",
      "(60, 37, 100, 176, 3) (60, 2) (6, 37, 100, 176, 3) (6, 2)\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Traning on batch\n",
    "\n",
    "data_directory = \"D:\\\\Magshimim\\\\Dataset\\\\20bn-jester-v1\"\n",
    "data_csv = \"D:\\\\Magshimim\\\\Train.csv\"\n",
    "val_csv = \"D:\\\\Magshimim\\\\Validation.csv\"\n",
    "chosen_labels = [14, 15]\n",
    "log_file = \"model_log.txt\"\n",
    "model_saves = \"model checkpoints\"\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = 'categorical_crossentropy'\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Training loop using train_on_batch\n",
    "num_epochs = 10\n",
    "batch_size = 30  # Adjust batch size as needed\n",
    "print(\"starting...\")\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "\n",
    "data_dict = get_csv_dict(data_csv, chosen_labels)\n",
    "val_dict = get_csv_dict(val_csv, chosen_labels)\n",
    "\n",
    "load_model_from_dir(model_saves, model)\n",
    "\n",
    "accuracy_list = np.array([])\n",
    "val_accuracy_list = np.array([])\n",
    "\n",
    "for batch in range(int(len(data_dict[chosen_labels[0]]) / batch_size)):\n",
    "    data = np.array([])\n",
    "    data_labels = np.array([])\n",
    "    test = np.array([])\n",
    "    test_labels = np.array([])\n",
    "\n",
    "    data_index = 0\n",
    "    val_index = 0\n",
    "    \n",
    "    for label in chosen_labels:\n",
    "        data_batch, data_labels_batch, data_index = get_batch(data_directory, data_dict, chosen_labels, label, batch_size, data_index)\n",
    "        test_batch, test_labels_batch, val_inedx = get_batch(data_directory, val_dict, chosen_labels, label, int(batch_size / 10), val_index)\n",
    "        \n",
    "        data = append_or_copy_array(data, data_batch)\n",
    "        data_labels = append_or_copy_array(data_labels, data_labels_batch)\n",
    "        test = append_or_copy_array(test, test_batch)\n",
    "        test_labels = append_or_copy_array(test_labels, test_labels_batch)\n",
    "        \n",
    "    data = tf.convert_to_tensor(data)\n",
    "    data_labels = tf.convert_to_tensor(data_labels)\n",
    "    test = tf.convert_to_tensor(test)\n",
    "    test_labels = tf.convert_to_tensor(test_labels)\n",
    "    \n",
    "    print(data.shape, data_labels.shape, test.shape, test_labels.shape)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            print(i)\n",
    "            batch_data = data[i:i+batch_size]\n",
    "            batch_labels = data_labels[i:i+batch_size]\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(batch_data)\n",
    "                if loss_fn == 'categorical_crossentropy':\n",
    "                    loss = tf.keras.losses.categorical_crossentropy(batch_labels, predictions)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid loss function specified.\")\n",
    "\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            total_loss += loss.numpy() \n",
    "            correct_predictions += np.sum(np.argmax(predictions, axis=1) == np.argmax(batch_labels, axis=1))\n",
    "\n",
    "        average_loss = total_loss / (len(data) / batch_size)\n",
    "        accuracy = correct_predictions / len(data)\n",
    "\n",
    "        # Validation\n",
    "        val_predictions = model(test)\n",
    "        val_loss = tf.keras.losses.categorical_crossentropy(test_labels, val_predictions).numpy()\n",
    "        val_accuracy = np.sum(np.argmax(val_predictions, axis=1) == np.argmax(test_labels, axis=1)) / len(test)\n",
    "\n",
    "        save_model(model, model_saves, log_file)\n",
    "\n",
    "        accuracy_list = np.append(accuracy_list, accuracy)\n",
    "        val_accuracy_list = np.append(val_accuracy_list, val_accuracy)\n",
    "        print(f'Epoch {epoch+1}, Loss: {average_loss}, Accuracy: {accuracy}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0499ebaa-c98e-45b0-b1d4-a72cf0950c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the model \n",
    "model.save('Model_test.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada3c38-6dee-4ee0-b643-b2f63a388ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph the accuracy\n",
    "# Plot the data\n",
    "plt.plot(accuracy_list)\n",
    "plt.plot(val_accuracy_list)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Epoch')\n",
    "plt.title('Modle Accuracy Plot')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
