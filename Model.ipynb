{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22114eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e6a3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "def read_from_file(file_name):\n",
    "    return np.load(file_name)\n",
    "    \n",
    "def generate_data_labels(labels, label, length):\n",
    "    data_label = []\n",
    "    for i in range(length):\n",
    "        result = [1 if labels[j] == label else 0 for j in range(len(labels))]\n",
    "        data_label.append(result)\n",
    "    return np.array(data_label)\n",
    "\n",
    "def read_from_batch(data_directory, chosen_labels, label, batch_num):\n",
    "    # FOR NOW list(read_from_file().values())\n",
    "    print(\"start reading data...\") \n",
    "    \n",
    "    data = read_from_file(data_directory + \"data\" + \"_batch_\" + str(batch_num) + \".npy\")\n",
    "    data_labels = generate_data_labels(chosen_labels, label, len(data))\n",
    "    print(\"read data\")\n",
    "    \n",
    "    test = read_from_file(data_directory + \"validation\" + \"_batch_\" + str(batch_num) + \".npy\")\n",
    "    test_labels = generate_data_labels(chosen_labels, label, len(test))\n",
    "    print(\"read validation\")\n",
    "    \n",
    "    return data, data_labels, test, test_labels\n",
    "\n",
    "def append_or_copy_array(base_array, new_array):\n",
    "    print(base_array.shape, new_array.shape)\n",
    "    if base_array.size == 0:\n",
    "        result_array = new_array\n",
    "    else:\n",
    "        result_array = np.concatenate((base_array, new_array), axis=0)\n",
    "\n",
    "    return result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14bc9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model architecture\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', input_shape=(37, 100, 176, 3)),\n",
    "    tf.keras.layers.MaxPooling3D((2, 2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),  # Add dropout with a rate of 25%\n",
    "    tf.keras.layers.Conv3D(64, (3, 3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling3D((2, 2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),  # Add another dropout layer with the same rate\n",
    "    tf.keras.layers.Conv3D(64, (3, 3, 3), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca8075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n",
      "start reading data...\n",
      "read data\n",
      "read validation\n",
      "60\n",
      "(0,) (300, 37, 100, 176, 3)\n",
      "(0,) (300, 5)\n",
      "(0,) (30, 37, 100, 176, 3)\n",
      "(0,) (30, 5)\n",
      "start reading data...\n",
      "read data\n",
      "read validation\n",
      "65\n",
      "(300, 37, 100, 176, 3) (300, 37, 100, 176, 3)\n",
      "(300, 5) (300, 5)\n",
      "(30, 37, 100, 176, 3) (30, 37, 100, 176, 3)\n",
      "(30, 5) (30, 5)\n",
      "start reading data...\n",
      "read data\n",
      "read validation\n",
      "70\n",
      "(600, 37, 100, 176, 3) (300, 37, 100, 176, 3)\n",
      "(600, 5) (300, 5)\n",
      "(60, 37, 100, 176, 3) (30, 37, 100, 176, 3)\n",
      "(60, 5) (30, 5)\n",
      "start reading data...\n",
      "read data\n",
      "read validation\n",
      "75\n",
      "(900, 37, 100, 176, 3) (300, 37, 100, 176, 3)\n",
      "(900, 5) (300, 5)\n",
      "(90, 37, 100, 176, 3) (30, 37, 100, 176, 3)\n",
      "(90, 5) (30, 5)\n",
      "start reading data...\n",
      "read data\n",
      "read validation\n",
      "80\n",
      "(1200, 37, 100, 176, 3) (300, 37, 100, 176, 3)\n",
      "(1200, 5) (300, 5)\n",
      "(120, 37, 100, 176, 3) (30, 37, 100, 176, 3)\n",
      "(120, 5) (30, 5)\n",
      "(1500, 37, 100, 176, 3) (1500, 5) (150, 37, 100, 176, 3) (150, 5)\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "120\n",
      "150\n",
      "180\n",
      "210\n",
      "240\n",
      "270\n",
      "300\n",
      "330\n",
      "360\n",
      "390\n",
      "420\n",
      "450\n",
      "480\n",
      "510\n",
      "540\n",
      "570\n",
      "600\n",
      "630\n",
      "660\n",
      "690\n",
      "720\n",
      "750\n",
      "780\n",
      "810\n",
      "840\n",
      "870\n",
      "900\n",
      "930\n",
      "960\n",
      "990\n",
      "1020\n",
      "1050\n",
      "1080\n",
      "1110\n",
      "1140\n",
      "1170\n",
      "1200\n",
      "1230\n",
      "1260\n",
      "1290\n",
      "1320\n",
      "1350\n",
      "1380\n",
      "1410\n",
      "1440\n",
      "1470\n"
     ]
    }
   ],
   "source": [
    "## Traning on batch\n",
    "\n",
    "data_directory = 'D:\\\\Magshimim\\\\Dataset\\\\batches 5 multiplier\\\\'\n",
    "chosen_labels = [12, 13, 14, 15, 16]\n",
    "batch_multiplier = 5\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = 'categorical_crossentropy'\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Training loop using train_on_batch\n",
    "num_epochs = 5\n",
    "batch_size = 30  # Adjust batch size as needed\n",
    "print(\"starting...\")\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "\n",
    "for batch in range(batch_multiplier):\n",
    "    data = np.array([])\n",
    "    data_labels = np.array([])\n",
    "    test = np.array([])\n",
    "    test_labels = np.array([])\n",
    "    \n",
    "    for label in chosen_labels:\n",
    "        data_batch, data_labels_batch, test_batch, test_labels_batch = read_from_batch(data_directory, chosen_labels, label, (label * batch_multiplier + batch))\n",
    "        print(label * batch_multiplier + batch)\n",
    "        \n",
    "        data = append_or_copy_array(data, data_batch)\n",
    "        data_labels = append_or_copy_array(data_labels, data_labels_batch)\n",
    "        test = append_or_copy_array(test, test_batch)\n",
    "        test_labels = append_or_copy_array(test_labels, test_labels_batch)\n",
    "        \n",
    "    data = tf.convert_to_tensor(data)\n",
    "    data_labels = tf.convert_to_tensor(data_labels)\n",
    "    test = tf.convert_to_tensor(test)\n",
    "    test_labels = tf.convert_to_tensor(test_labels)\n",
    "    \n",
    "    print(data.shape, data_labels.shape, test.shape, test_labels.shape)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            print(i)\n",
    "            batch_data = data[i:i+batch_size]\n",
    "            batch_labels = data_labels[i:i+batch_size]\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(batch_data)\n",
    "                if loss_fn == 'categorical_crossentropy':\n",
    "                    loss = tf.keras.losses.categorical_crossentropy(batch_labels, predictions)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid loss function specified.\")\n",
    "\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            total_loss += loss.numpy() \n",
    "            correct_predictions += np.sum(np.argmax(predictions, axis=1) == np.argmax(batch_labels, axis=1))\n",
    "\n",
    "        average_loss = total_loss / (len(data) / batch_size)\n",
    "        accuracy = correct_predictions / len(data)\n",
    "\n",
    "        # Validation\n",
    "        val_predictions = model(test)\n",
    "        val_loss = tf.keras.losses.categorical_crossentropy(test_labels, val_predictions).numpy()\n",
    "        val_accuracy = np.sum(np.argmax(val_predictions, axis=1) == np.argmax(test_labels, axis=1)) / len(test)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {average_loss}, Accuracy: {accuracy}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaa6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the model \n",
    "model.save('Model_test.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
