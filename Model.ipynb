{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22114eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e6a3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "def read_from_folder(directory, input_shape):\n",
    "    input = np.empty(input_shape, dtype='uint8')\n",
    "    image_count = 0\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        if os.path.isfile(f) and (f.endswith('.jpg') or f.endswith('.jpeg')):\n",
    "            img = Image.open(f)\n",
    "    \n",
    "            # Resize\n",
    "            size = 176, 100\n",
    "            img = img.resize(size, Image.LANCZOS)\n",
    "    \n",
    "            numpydata = np.array(img)\n",
    "            input[image_count] = numpydata\n",
    "    \n",
    "            image_count += 1\n",
    "    \n",
    "            if image_count == 37:\n",
    "                break  # Stop processing if 37 images are reached\n",
    "    \n",
    "    while image_count < 37:\n",
    "        input[image_count] = input[image_count - 1]\n",
    "        image_count += 1\n",
    "    \n",
    "    return input\n",
    "\n",
    "def get_csv_dict(directory, labels, desired_rows):\n",
    "    data = pd.read_csv(directory)\n",
    "    data_dict = {i: [] for i in labels}\n",
    "    \n",
    "    for label in labels:\n",
    "        filtered_data = data[data['label_id'].isin([label])]\n",
    "\n",
    "        if len(filtered_data) >= desired_rows:\n",
    "            data_dict[label] =  filtered_data['video_id'].values[:desired_rows]\n",
    "            continue\n",
    "    \n",
    "        rows_to_add = desired_rows - len(filtered_data)\n",
    "        random_rows = filtered_data.sample(n=rows_to_add, replace=True)\n",
    "        data_dict[label] = pd.concat([filtered_data, random_rows], ignore_index=True)['video_id'].values\n",
    "\n",
    "    return data_dict\n",
    "        \n",
    "def generate_data_labels(labels, label, length):\n",
    "    data_label = []\n",
    "    for i in range(length):\n",
    "        result = [1 if labels[j] == label else 0 for j in range(len(labels))]\n",
    "        data_label.append(result)\n",
    "    return np.array(data_label)\n",
    "\n",
    "def get_batch(directory, data_dict, chosen_labels, label, amount, index):    \n",
    "    data = np.array([])\n",
    "\n",
    "    for i in range(index, (index+amount)):\n",
    "        example = read_from_folder(f\"{directory}/{data_dict[label][i]}\", (37, 100, 176, 3))\n",
    "        example = np.expand_dims(example, axis=0)\n",
    "        data = append_or_copy_array(data, example)\n",
    "    \n",
    "    #data = read_from_file(data_directory + \"data\" + \"_batch_\" + str(batch_num) + \".npy\")\n",
    "    data_labels = generate_data_labels(chosen_labels, label, len(data))\n",
    "    \n",
    "    return data, data_labels\n",
    "\n",
    "def append_or_copy_array(base_array, new_array):\n",
    "    if base_array.size == 0:\n",
    "        result_array = new_array\n",
    "    else:\n",
    "        result_array = np.concatenate((base_array, new_array), axis=0)\n",
    "\n",
    "    return result_array\n",
    "\n",
    "def load_model_names(log_file):\n",
    "    model_names = set()\n",
    "\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, 'r') as file:\n",
    "            model_names = set(file.read().splitlines())\n",
    "\n",
    "    return model_names\n",
    "\n",
    "def cleanup_models(save_dir, log_file, model_names):\n",
    "    while len(model_names) > 10:\n",
    "        oldest_model = min(model_names)\n",
    "        oldest_model_path = os.path.join(save_dir, oldest_model)\n",
    "\n",
    "        # Delete the oldest model file\n",
    "        if os.path.exists(oldest_model_path):\n",
    "            os.remove(oldest_model_path)\n",
    "\n",
    "        # Remove the oldest model name from the set and the log file\n",
    "        model_names.remove(oldest_model)\n",
    "        with open(log_file, 'w') as file:\n",
    "            file.write('\\n'.join(model_names) + '\\n')\n",
    "\n",
    "def save_model(model, save_dir, log_file, model_name_prefix='model'):\n",
    "    # Generate a unique model name\n",
    "    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    model_name = f'{model_name_prefix}_{timestamp}'\n",
    "\n",
    "    # Save the model\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    model.save(model_path)\n",
    "\n",
    "    # Update the model names set\n",
    "    model_names = load_model_names(log_file)\n",
    "    model_names.add(model_name)\n",
    "\n",
    "    # Update the log file with the new model name\n",
    "    with open(log_file, 'a') as file:\n",
    "        file.write(model_name + '\\n')\n",
    "\n",
    "    # Remove oldest model if exceeding the maximum allowed models\n",
    "    #cleanup_models(save_dir, log_file, model_names)\n",
    "\n",
    "    print(f'Model saved: {model_name}')\n",
    "\n",
    "def load_model(model_path):\n",
    "    return tf.keras.models.load_model(model_path)\n",
    "\n",
    "def load_model_from_dir(save_dir, model):\n",
    "    saved_models = [f for f in os.listdir(save_dir) if f.startswith('model_')]\n",
    "\n",
    "    if not saved_models:\n",
    "        print('No saved models found.')\n",
    "        return model\n",
    "\n",
    "    print('Saved Models:')\n",
    "    for i, model_name in enumerate(saved_models, start=1):\n",
    "        print(f'{i}. {model_name}')\n",
    "\n",
    "    choice = input('Do you want to load a model? (y/n): ').lower()\n",
    "\n",
    "    if choice == 'y':\n",
    "        model_choice = int(input('Enter the number of the model to load: '))\n",
    "        if 1 <= model_choice <= len(saved_models):\n",
    "            selected_model = saved_models[model_choice - 1]\n",
    "            model_path = os.path.join(save_dir, selected_model)\n",
    "            loaded_model = load_model(model_path)\n",
    "            print(f'Model {selected_model} loaded successfully.')\n",
    "            return loaded_model\n",
    "        else:\n",
    "            print('Invalid choice. No model loaded.')\n",
    "            return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14bc9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', input_shape=(37, 100, 176, 3)),\n",
    "    tf.keras.layers.MaxPooling3D((2, 2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv3D(64, (3, 3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling3D((2, 2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv3D(128, (3, 3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling3D((2, 2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv3D(256, (2, 2, 2), activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    tf.keras.layers.Dense(7, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774607ef-9225-4f0b-a7cc-7992c939e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traning on batch\n",
    "\n",
    "data_directory = \"/tf/data/20bn-jester-v1\"\n",
    "data_csv = \"/tf/data/updated_train.csv\"\n",
    "val_csv = \"/tf/data/updated_validation.csv\"\n",
    "chosen_labels = [0, 19, 20, 21, 23, 24, 25]\n",
    "log_file = \"model_log.txt\"\n",
    "\n",
    "log_accuracy_file = \"model_accuracy_log.txt\"\n",
    "model_saves = \"/tf/project but works/checkpoints\"\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = 'sparse_categorical_crossentropy'\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Training loop using train_on_batch\n",
    "num_epochs = 25\n",
    "batch_size = 10  # Adjust batch size as needed\n",
    "val_percentage = 10\n",
    "desired_examples = 4000\n",
    "print(\"starting...\")\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "\n",
    "data_dict = get_csv_dict(data_csv, chosen_labels, desired_examples)\n",
    "val_dict = get_csv_dict(val_csv, chosen_labels, int(desired_examples / val_percentage))\n",
    "\n",
    "print(len(data_dict[0]), len(val_dict[0]))\n",
    "\n",
    "load_model_from_dir(model_saves, model)\n",
    "\n",
    "accuracy_list = np.array([])\n",
    "val_accuracy_list = np.array([])\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    data_index = 0\n",
    "    val_index = 0\n",
    "    \n",
    "    for data_index in range(0, desired_examples, batch_size):\n",
    "        data = np.array([])\n",
    "        data_labels = np.array([])\n",
    "        test = np.array([])\n",
    "        test_labels = np.array([])\n",
    "        \n",
    "        for label in chosen_labels:\n",
    "            data_batch, data_labels_batch = get_batch(data_directory, data_dict, chosen_labels, label, batch_size, data_index)\n",
    "            test_batch, test_labels_batch = get_batch(data_directory, val_dict, chosen_labels, label, int(batch_size / val_percentage), val_index)\n",
    "            \n",
    "            data = append_or_copy_array(data, data_batch)\n",
    "            data_labels = append_or_copy_array(data_labels, data_labels_batch)\n",
    "            test = append_or_copy_array(test, test_batch)\n",
    "            test_labels = append_or_copy_array(test_labels, test_labels_batch)\n",
    "            \n",
    "        data = tf.convert_to_tensor(data)\n",
    "        data_labels = tf.convert_to_tensor(data_labels)\n",
    "        test = tf.convert_to_tensor(test)\n",
    "        test_labels = tf.convert_to_tensor(test_labels)\n",
    "        \n",
    "        print(data.shape, data_labels.shape, test.shape, test_labels.shape, data_index, val_index)\n",
    "        \n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            print(i, data.shape)\n",
    "            data_example = data[i:i+batch_size]\n",
    "            labels_example = data_labels[i:i+batch_size]\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(data_example)\n",
    "                if loss_fn == 'sparse_categorical_crossentropy':\n",
    "                    loss = tf.keras.losses.categorical_crossentropy(labels_example, predictions)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid loss function specified.\")\n",
    "\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            total_loss += loss.numpy() \n",
    "            correct_predictions += np.sum(np.argmax(predictions, axis=1) == np.argmax(labels_example, axis=1))\n",
    "\n",
    "        average_loss = total_loss / (len(data) / batch_size)\n",
    "        accuracy = correct_predictions / len(data)\n",
    "\n",
    "        # Validation\n",
    "        val_predictions = model(test)\n",
    "        val_loss = tf.keras.losses.categorical_crossentropy(test_labels, val_predictions).numpy()\n",
    "        val_accuracy = np.sum(np.argmax(val_predictions, axis=1) == np.argmax(test_labels, axis=1)) / len(test)\n",
    "\n",
    "        accuracy_list = np.append(accuracy_list, accuracy)\n",
    "        val_accuracy_list = np.append(val_accuracy_list, val_accuracy)\n",
    "\n",
    "        # Update the log file with the new model name\n",
    "        with open(log_accuracy_file, 'a') as file:\n",
    "            file.write(f'Epoch {epoch+1}, Loss: {average_loss}, Accuracy: {accuracy}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}' + '\\n\\n')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {average_loss}, Accuracy: {accuracy}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}')\n",
    "        val_index += int(batch_size / val_percentage)\n",
    "        \n",
    "    save_model(model, model_saves, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a7e7b-8534-43f1-a01e-9a12c84b9159",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, model_saves, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada3c38-6dee-4ee0-b643-b2f63a388ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph the accuracy\n",
    "# Plot the data\n",
    "plt.plot(accuracy_list)\n",
    "plt.plot(val_accuracy_list)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Epoch')\n",
    "plt.title('Modle Accuracy Plot')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
