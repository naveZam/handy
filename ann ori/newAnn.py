# -*- coding: utf-8 -*-
"""
Created on Tue Mar  9 23:57:11 2021
Machine Learning
exercise NN 2023
"""

import numpy as np
import matplotlib.pyplot as plt
import scipy.io as sio
import pandas as pd
import time


def sigmoid(Z):
    """
    Compute the sigmoid of Z
    Input Arguments:
    Z - A scalar or numpy array of any size.
    Return: A - sigmoid(Z)
    """
    A = 1 / (1 + np.exp(-Z))

    return A


def ReLU(Z):
    """
    Compute ReLU(Z)
    Input Arguments:
    Z - A scalar or numpy array of any size.
    Return:
    A - ReLU(Z)
    """
    A = np.maximum(0, Z)

    return A


def dReLU(Z):
    """
    Compute dReLU(Z)
    Input Arguments:
    Z - A scalar or numpy array of any size.
    Return:
    A - dReLU(Z)
    """
    A = (Z > 0) * 1

    return A


def init_parameters(Lin, Lout):
    """
    Init_parameters randomly initialize the parameters of a layer with Lin
    incoming inputs and Lout outputs
    Input arguments:
    Lin - the number of incoming inputs to the layer (not including the bias)
    Lout - the number of output connections
    Output arguments:
    Theta - the initial weight matrix, whose size is Lout x Lin+1 (the +1 is for the bias).
    Usage: Theta = init_parameters(Lin, Lout)

    """

    factor = np.sqrt(6 / (Lin + Lout))
    Theta = np.zeros((Lout, Lin + 1))
    Theta = 2 * factor * (np.random.rand(Lout, Lin + 1) - 0.5)
    return Theta


def ff_predict(Weights, X, y):
    """
    ff_predict employs forward propagation on a 3 layer networks and
    determines the labels of  the inputs
    Input arguments
    Theta1 - matrix of parameters (weights)  between the input and the first hidden layer
    Theta2 - matrix of parameters (weights)  between the hidden layer and the output layer (or
          another hidden layer)
    X - input matrix
    y - input labels
    Output arguments:
    p - the predicted labels of the inputs
    Usage: p = ff_predict(Theta1, Theta2, X)
    """
    m = X.shape[0]
    num_outputs = Weights[-1].shape[0]
    p = np.zeros((m, 1))
    a = X
    for Weight in Weights:
        a0 = np.ones((a.shape[0], 1))
        a = np.concatenate((a0, a), axis=1)
        z = np.dot(a, Weight.T)
        a = sigmoid(z)
    p = np.argmax(a.T, axis=0)
    p = p.reshape(p.shape[0], 1)
    detectp = np.sum(p == y) / m * 100

    return p, detectp


def backprop(Weights, X, y, max_iter=1000, alpha=0.9, Lambda=0):
    """
    backprop - BackPropagation for training a neural network
    Input arguments
    Theta1 - matrix of parameters (weights)  between the input and the first
        hidden layer
    Theta2 - matrix of parameters (weights)  between the hidden layer and the
        output layer (or another hidden layer)
    X - input matrix
    y - labels of the input examples
    max_iter - maximum number of iterations (epochs).
    alpha - learning coefficient.
    Lambda - regularization coefficient.

    Output arguments
    J - the cost function
    Theta1 - updated weight matrix between the input and the first
        hidden layer
    Theta2 - updated weight matrix between the hidden layer and the output
        layer (or a second hidden layer)

    Usage:
    [J,Theta1,Theta2] = backprop(Theta1, Theta2, X,y,max_iter, alpha,Lambda)
    """

    m = X.shape[0]
    J = 0
    weightsGrad = []
    weightsDer = []
    numOfLayers = len(Weights)
    deltas = np.ones(numOfLayers)
    for q in range(max_iter):
        J = 0
        weightsGrad = []
        weightsDer = []
        for i in range(numOfLayers):
            weightsGrad.append(np.zeros(Weights[i].shape))
            weightsDer.append(np.zeros(Weights[i].shape))

        r = np.random.permutation(m)

        for k in range(m):
            for i in range(numOfLayers):
                weightsGrad[i] = np.zeros(Weights[i].shape)
                weightsDer[i] = np.zeros(Weights[i].shape)
            trainingExample = X[r[k], :]
            trainingExample = trainingExample.reshape(1, trainingExample.shape[0])
            ### Forward propagation
            bias = np.ones((trainingExample.shape[0], 1))
            trainingExample = np.concatenate((bias, trainingExample), axis=1)
            trainingExample = trainingExample.T
            a = [np.array(trainingExample)]
            a_l = trainingExample
            for i in range(numOfLayers - 1):
                zL = np.dot(Weights[i], a_l)
                a_l = sigmoid(zL)
                a_l_0 = np.ones((a_l.shape[1], 1))
                a_l = np.concatenate((a_l_0, a_l), axis=0)
                a.append(a_l)
            i+=1
            zL = np.dot(Weights[i], a_l)
            a_l = sigmoid(zL)
            a.append(a_l)





            ### Backward propagation
            ybin = np.zeros(a[-1].shape)
            ybin[y[r[k]]] = 1  # Assigning 1 to the binary digit according to
            # the class (label) of the input
            J += 1 / m * (-1) * (np.dot(ybin.T, np.log(a[-1])) + np.dot((1 - ybin).T, np.log(1 - a[-1])))
            deltas = [np.array((a[-1] - ybin))]

            """for i in range(len(weights)):
                g_tag = a[i] * (1 - a[i])
                np.insert(deltas, 0, np.dot(Weights[i][:, 1:].T, deltas[i]) * g_tag[1:], axis=1)"""
            list_index = -1

            for aa in reversed(a[1:-1]):
                g_tag = aa * (1 - aa)
                deltas.insert(0, np.dot(Weights[list_index][:, 1:].T, deltas[list_index]) * g_tag[1:])

                list_index -= 1

            for i in range(len(deltas)-1):
                weightsGrad[i] += np.dot(deltas[i], a[i].T)

        #### Updating the parameters
        """for i in range(numOfLayers):
            Weights[i] = Weights[i] - alpha * weightsDer[i]"""
        dTheta_index = 0
        for dTheta in weightsGrad:
            weightsDer[dTheta_index] = 1 / m * dTheta
            weightsDer[dTheta_index][1:, :] = weightsDer[dTheta_index][1:, :] + Lambda / m * Weights[dTheta_index][1:,:]

            dTheta_index += 1

        #### Updating the parameters

        for i in range(len(Weights)):
            Weights[i] = Weights[i] - alpha * weightsDer[i]
            J += np.sum(Weights[i] ** 2) * (Lambda / (2 * m))

        J += (Lambda / (2 * m)) * (np.sum(Weights[0] ** 2) + np.sum(Weights[1] ** 2))
        # print(q)
        if np.mod(q, 2) == 0:
            print('Cost function J = ', J, 'in iteration',
                  q, 'with Lambda = ', Lambda)
            p, acc = ff_predict(Weights, X, y)
            # acc = np.sum(p==y) / m *100
            print('Net accuracy for training set = ', acc)
            # time.sleep(0.005)

    return J, Weights

from keras.datasets import mnist

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# for i in range(10):
#     plt.figure(1, figsize = (10,5))
#     plt.imshow(train_images[i])
#     plt.suptitle('label =' + str(train_labels[i]))
#     plt.show()
#     plt.pause(0.1)

print('train_images.shape =', train_images.shape)
print('train_labels.shape =', train_labels.shape)
print('test_images.shape =', test_images.shape)
print('test_labels.shape =', test_labels.shape)

# pre-processing
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype('float32') / 255

test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype('float32') / 255

# from keras.utils import to_categorical

# train_labels = to_categorical(train_labels)
# test_labels = to_categorical(test_labels)

X = train_images[:7200, :]
y = train_labels[:7200]
y = y.reshape((y.shape[0], 1))

L1 = X.shape[1]
num_of_units_on_hidden_layers = 16
num_output_units = np.unique(y).size
numOfLayers = int(input())
Weights = [np.array(init_parameters(L1, num_of_units_on_hidden_layers))]
for i in range(1, numOfLayers-1):
    Weights.append(init_parameters(num_of_units_on_hidden_layers, num_of_units_on_hidden_layers))
Weights.append(init_parameters(num_of_units_on_hidden_layers, num_output_units))
print("""..................................................................................................:.........................................:.................
.=================---------::::::::::::..::::.:::::::::::::::::::..:...::-----------::::::::..::.:.......:..:::::::::...:::::::::::::::::::::::::::::::::::::.
.====================-----:-::::::::::::::.:.::::::::::::..........:::::::---------:-::-::::::............:.::::..:.:::::::::::::::::::::::::::::::::::::::::.
.====================-----:-:::::::.:::::::::::::::........:+#%@@@@%%#+=-:..................::.......:::::::.::::::::::::::::::::::::::::::::::::::::::::::::.
.=============+======------:-::::::::::::::::::......:+#@@@@@%%#****+**##%##%%###**++=-:.............::::::::::::::::::::::::::::::::::::::::::::::::::::::::.
.====================------::::::::::::::::......-#@@@@@%%######%####%%@#%%%%%%@@@@@@@%%@@@@@%##+:.........::::::::.::::::::::::::::::::::::::::::::::::::::-.
.=====+==============-------::::::::::::::..:=##%@@%%#########**#*#%%##%#%##%#######****######%%@@@@@@@*-..::........::::::::::::::::::::::::::::::::::::::-:.
.====================------::::::::::::::..--+##%####*##%%%####%####*####################**########%%@%@@@@@@*....:......::::::::::::::::::::::::::::::::::::.
.==================-=-==-----:::::::::::..=#*%%%%%###%@#.=#%%%%##%%%###%%%##########*########*###***######%%%@@@@@*-..::....::::::::::::::::::::::::::::::::-.
.====================-----:----:-::::...-%@%%%%#*#%@%##**#*+*##%%#%%%%%%#%%%########*##**##*###*######*####*####%%@@@%+---.....::::::::::::::::::::::::::-:::.
.===============-====-=---------:::..:#@%%@@@@@@@@+::+***####%%%%%%######*****##*###########********#**#######****##%%@@@#+==:...::::::::::::::::::::::-:-:::.
.======-====-===========--------::.=*#*-..-:......+%@####%#%%%%###*#*#%#**%######**#***####*#**####***##**######*##*####%%@@%++-.:..:::::::::::::::::::::::--.
.======-=-==============-===-----=**+-..::.:...:#%*=-+*#%%%%%%%%#*##***#*#########%%%#%#*##**+##*+*#%######*#########**##*#*%@@@%#...:::::::::---:--:::::----.
.===-=====--================-----==:...::::..+@@#+=*#%%%%@%%%#%#**##%%%##%#*###%##*#%**###*#++#%#*######*#**###%*##*##*#######*#%@%*:...::-:::::::::---::--:-.
.===-=---========================-:::::-::..#%#**##%%#%%%#%###*#%#*+##*#####%#######**###***##**+=#****#%%#%#####%%#%######**##***%@%%:..::---::::-:::-:-----.
.==================+======+====--:-:::::..:*@####%##%%%%%%%%%##*##%%%#%###%%#%#%%####**####***###%%##%%****####*#####%#*##########**%@@#:.::--------------:--.
.=====================+=+======::---:-::..%%#####%%%%%%%#***##*####*#%#%%########%%############%%#######%%#####%####%#######**#*###**#%%@#...::----------:---.
.==============++=+++++++======::---:-:..#%%##%%%%%%##%####*#%@@##%%%@%##%@%%%#**#%%%%%%####*#######%#%**%%####*########%%%@%########***#%%-..:--------------.
.============+====++++++=====---=----:..@@%#%%#####%#%%%%##%%%###@%%%#*%%%####%@@%%###*#%#%####%%%##**###########%###%###########%*#####*#@@%...-------------.
.==============+++++++++=+=====-----:..%@#*#%##%%%%%%###%#%@%%%%%###%%%@%%%%#%%#*##**#*##%#***##%%%#*####%%#%%###%###%%%%%#%@%@@%#####*###*#%%=..:-----------.
:============+++=+++++++=+===-------:.+@######%##%%%##%%%@%%%##%@@@#%%#**#%##%%%@%%%%%%####%%%%%#####%%%%%#**###*####*+##%%%#*#@@@@@%##*****#%@%..:----------:
.=======+=++++++++++++++++===------:.:@%###%%%##%%##%%*#####%#%###%%#%%%%%@%@%%###*#%%%%@@####%#*#%#%#####%%@%#%%##%#%##%%##%#....+@@@@@@#*****%%-.:---------.
.===+===++++++++++++++++======-----..@###%###%%%%%%###%%%%%%##%@%@%%@%%%%%%%%#@@@@#%######%#######*########%########%###++###%@:........@@##***#%#-.:--------.
.=+==+=++++=++++**++++++=+====---=:.=%#%####%%%%%%%###%%**#%@@%%%%%%%@@%%%#%%%%%%#%#@@%%%%%%%####@%%#%#%%%%##%###%%######%**##@@..:---:..%@#***#*##=.:-------.
.==+=+=++++++++**+=++++++========-..%%%#%%%##########%%##%%#**##*###%%%#####%####%@#@%%%%%##%%%@##%%###%###%%%##%%%%%%%##**+*#%@%:------..*@#***+*#@*.:------.
.=+=+++++++++++**+++*=+++========-.*@#%###%####%##%%##%%%%%#%%%%%@###*##*%%%%%%%##****########%%#%%##%##%##%%%##%%###*####%#**##+..------..*@#**####%+.:-----.
.=+=++++++*++**+****+*++++++===--:.%@%##%%%##%***#*+**##*##%%%%%@@@@@@@@@%%#%#%@%%@%@@%####%%%%%####*#%%%%%%%%%%%%#####%%**+###%@*.-:-----..#@%%#***#%+.:--=-.
.=+++++++*+**+***++++++++++===-::..@@%%####%##%#**+++==++++*##****###%%%%%%%@%%%%%@%%#%%@@%%#%%%###%@%%%#%##**#%###%%*%%##%#*##*@*.:-=-----..*+%@@#***#=:--=-.
.=+=+++++++*********+++*+++++++#%..@%%#%%%%#*###***+++++++++***######%###%@%%%@%%#%#*######%%%%%%%%%@%######%%######%%####****##%@-::::-=---:....*@#**@@--=--.
.==++++++****#######*****++*#++%@@@@%#%#%#####*********+++++++++====+***+++++**#@@%%%%#%%%%####%%%%#%##%%####%%#%%@##**###%###*##@=:===-:-------..#@#*=::====.
.+++++++**##########***+**#%*=#@@@@@@%%%%%###%##**++*++**#**++=++++++==++++==+++++#%%@%%#%@@%%%*#%%#%#####%%########%%%##%#*##*%#@*.:------------:.%*=.:-====.
.++++=++#*################%#+*%@%%%%%%%##%%###**+#**++*+++==+*++++++=++++++++++==+*+++*%%#*#%%%@%%%#%@%%%%########%%%###%#****%##@@.:-=-=--=------.+@*=:--===.
.++++=+**########%##%%%%%#***%%%%%%%%%%%%%#%%#*+++++**+++******+++*++++++++=++++*++=+*#*###*+***#########%%#%##%%####%%%####%#**#@%.:-----=-------:.##+:---==:
.+=++++**######%%%###%%%*.*@@@%@%%#%%%%%%####+******+++*+***++++++*+++++++****++=-+++===++==+++++++*+*****##%%#%%%#########%##%##%@.--------=---:...=#****+-=.
.++++=***#####%#%##%#####%@@@%%%%%%%@@%%#********+********++++***++-++++=+++++++**+++++++++*#*++++++**+**#####%%@%%%%%%%%#%####*#%%.:----=-------+@@%#*++*+==.
.=++++***#####%%%%#%##@@@@@%%%@%%%%%%%***#**+*****++***++**+***+++**+++*****++***#*++**++*+*++++*++******+++*+*####%##%%#%%%%###*@#:--==-----=---:@#********=.
.+++=+***###%%%%##%%%#@%%%#%#%%#%%%%@%*******+*****+*++**+++++++*+++*+==+*****++++++*+++**++*+*+*##**++++*#+*+++++**####*##*#####@#.------==-=-=-:=%****++**=.
.====+***#########%%%@%%@%%@%%%%@%%#%%**+*+*****++++++++++=++++***++++**+++++++*+**+*+++++*******************#****##*****+**###*#@+.-==---=-=-=---:%#*******=.
.-==+#*###%%%%%#%%#%#%%#%%#%%%%%#%%%@********+++=+++**++++**++++===+++++=+==+***++**+*****+**++****+*+++****+*+************##**#@%.-===-=---=---+=:##********.
.-==+#***#%#%%%#%%%%##%@%#%@%%%%%%%@@*++*+***##%#%%%#***+*+=+*=++**++++++=++*+=++++++++=+*++*+**+++*#*******#***********+##%##*%@=.---=--====-=-==-##********.
.-=+****#%%%%%####%%%@@#%@@@%%%%@%@@#+**#%@@@@@@@@@@@@@@%##*****+++=**+++++**++++*+++**+++=+++++**++***++********#***##*#######@#:-====---=-=-====-*#*******#.
.==++*####%%%%%%%@%%%%%%#%##%%###%%%#***%####%@@@@@@@@@@@@@%%%%**++++++*+*+=+*+==+***++++***++++*##*****#***#*******##*#######@@::-=-===--===-====-##********.
.==++###%%%%%###%%%%%@%#%@@%%%%%%@%*+*##***++=====+++#%@@@@@@%@@%#++++++++==++++*++=+++++++++***++*+*+++**********#****#####**%..:==--=====-=-===-=%##**##**#.
.+=+++*####%%%%%%%#%%%#%@%%%%%#%%@%*****#**+++===+=-==-==+#%@@@%%@%%#*++*+*++==+=++++=++***+#****#**#***#***##****####**#%###*%%=..:---===-======:*%**#**#***.
.=-+###%%%%%%##%%%@@@%%%%%@@%%%@%@**********+**#**+*++=-----+*%@@@@@@%#****+**+#***++++**++++**+++++*+**+***##**#*#*%*###%%#%#%%@@#:.:--========-:@######**##.
.=-%##%%%###%%##%#%%%%%%%%%%%%%%@******+*+##%##%@@@@@@@@@@@*====+#%%@@@%*++++***++*#*+++******++**++*****#++*+**#*###*###%%#%%###%@@+=+...--===-:=@##########.
.--%##%#%@%##%%%%%%%%#%%%%%%##%@#*****++*****%@@@%#**%@@+=%@@@@#+-==+#%#####****+++++*****++=+++++*+**+****######*##%##%%%#%%%%%###%@#*@%+::-=--=%##%%###****.
.=*%%%##%###%%%#*#%%%#%%@%%%%%%%#####********+**#%#*::#@@@@@%+#%%#%**++******++****#******#%%%@%%%%%######**#***#***#%##%@@%%%%##%%##@=%++**:..=@%##%%%######.
.###*#%#%%#%##%%%%%%#@@%#%%#%%@%***++*+*****#**+++*%@@%%%@%*+**#%#*###**#*+****##****#%%%%%%%@@@@@@@@@@@@@@@@@@@%########%%%%%%%%%%%%@=%@@@@@@@@@%@%%@%%#####.
.%#%#%%##*%#%#%%#%%%%%%%@@%%%#%#*****##**+*****+*****+*##**#%%@#%%*****#******+=*******#%%%%@@@@@@@@@@@@@@@@@@%%@%%#*#%##%@%%%%%%%@%%@#@@#@@@@@%%@%%%%%%%###*.
.%%###%%##%#####%%%%%%%%#%%%%%@#******++++*++***********+++**++==++*****#****++********#**+++==++=**+**#%%@@@@@@@@@@@@#%@@@%%%%%%@%%%@%%@@%@%@%@%%@@@%%%####+.
.%%########%%%%%%%%#%%%%%@@@%@%*#****++**++++**+**+************#****#**####*++**+*######**+###***++=========+*##%@@@@@*#@%%%%%@@%@@@@%%%%%%%%%%%#%##%%%%#***+.
.@@@%%%*####%###%%%%%%%%##%##%%****+***+++++*+++++++*++*************#*##*****+***######**%%##@@@@@@@@@@%##%#***#**#%%@#@%%%@@%%%@%%%%%%%%@@%%@%%%@@%%%%%##**+.
..:+****#######%#%%%%#%%%%#%%@%****#*+*+*##***********+++**+******#*#*****++++***##*#***%@@@#+-=%%%%%@@@@@##%###**#####%@%%%@@@%%%@@@@%@%%%%@%%%%%%@@%%##**++.
.....:+######%%%%%#%%#%%%@%%%@%**#*****##++++**++***+++*++*+**+****##********+*+**####****+*#%@%%@@@@@#+#@@@%%##%#%###%@@@%@@%%@%@%%#%%%%%%@%%%%%%%@###%##*++.
.---.-**######%%%#%%##%%###%%%#***+**+++++*+********++****++*************+++*++*##%#######***#*####%%%%@%%@@@%%%#####%@@@@@%%%%@@@%@@@@@@%@%%@%@%%%%%%###*+++.
.---:-*#%##%##%##%%@%%%%%#%%%@#+**+*++******##***+++++++*++*##*****+*******+********##*##*##*####*##########%@%####%##@@%%%@@@%%%%%%%%%%%@@@%#%%%%%%%%%##=+++:
.---::+###%%%%%%%%%%####%#%%%%************++++**++*****+*+++*******#***#***+*+****#####**####*+*#*#*****###*****#####%@@@@@@@@%@@@@@@@%%%@%@@@@%#%%%%##%*+*+*.
.----:+%%%%@@@@@%%%%%%%%%%%%%#*********++++#**+++++++*++**+*******##****+++******#***#***#**####*#**#*****#########*#%@@%%%%@@%%%%%%@@%@@@%%%%%%%%%%##*+++*+*.
.----::--=++#*####%%%%%%%%%%@%#*******++**+++++*****+**+*****+**#*******+++****#*#**#**#***#**##*#*#**######*####%%#%@@%@@@%%@@%@@@@%%%@@%%%@@@%%%%%##*=++*++.
:------:::-:-=::*##*#%%%%%%%@#****++*****++****+**+*++++*+++*###**********#******#**###****+#******#****############%@@@%@@@@%#%@%@%#%@@@%%@@%%@@%%%%#++++*+*.
.----------::::::::==++++*%@@#++++***++++**+*+++++*+++*****#%###*********+*+******#**##***#******#***#******####*#*%@@%@@%%@@%%@@@@#%@@@%%@@#%%@%%#%*=+**+*++.
.------------=====-:::--=---:=+**+*+*+***##******%###****#%#**#**********+*+***#***#******#****#######*###########%@%%%@%@@%%@@@%@@%@@%@%%%#@@%%%%#*++*+*+***.
.---------=--=---=-===--:::::=+********++**********+++*##*#*********************%#*#**#*****#***#*##%%%*#*#%###%%#@@@@@%%@@%%@@@@%%%%%%@@%%@@%%@%%#++*****+*+.
.-----------------==-=-=====-=+++****+**##****++*+*********++***+**+**#*+*******#*#%#***###*#####%#*#############@@@@@@@@@@@@%%@@%@@@@@@@@@@@%@%@#+++**+*+**+.
.--------------=----=-=-===--=+***************#*+***++****#**++****##*++***##*##*#######**#***##****##**########@@@@%@@@%%%@@%@%%%@@%%%@%%@@%%#*+=+****+*+***.
.--------------------==-====--+******+**************#*****##%##**********##****##***###****##*****#**###*##**##%@%@@@@%@@@%@@%@%@@@@@@@@%%%%@%%*+*********+*+.
.---------------------===-==--+*****#**********+***++**#*****#####++++=++*******#*###**#####**###***#####%##%#%@@@%%%@@@@@@@@@@@@@%%%%%@%%%%%#*++*******+****.
.------------=-----=-===-====:=+****#**##***+*+*****##***##****###******+++++****######****####**#*#%#####**%@@@@@@@@@@@%%@@@%%@@@@@%@%@@%%%#*++*************.
.--------------=-==-======-:..=##*#*****+**#*#%@%%%%%####%########**+****+**#%##%#######*******###*####*##**#@%%@@@@@@@@@@@@@%@@@@@@%@@%%@%#*++********+***+*.
.=-------=-----==-=-==--:...:#@#**#********##%%%#**##*##*##*#**##*########%%%#%###**#**###*#####*####**####%@%@@@%%%%@@%@@@@%%@@@@%@%@#%%@%#*+***************.
.-==-======-==-=--==--:..-%@@@@#**#***#**##*#***#%%*****####%#*#*#%#%%%#########+**#*#######*#####*######%%@@@%%@@@@%%@@@@%%@@@%%%%%%@@@%%%*+****************.
.===-=-===========--:..*@@@@@@%+*********#**#####%%##%%**+++***###*##*#########**#####*##########**####%%#@@@@@@@%@@@@@@@%@@@@@@@%@@@%#%##*++****************.
.-===========-=----:-*@@@%@@@*+*#####***####******####%@%#%%%#***###########%%#%##**###########%%%%%##%##@@@@@@@@@%%%%%%@%@@@@%@@%@%%%%##********************.
.============----=+*%@@%@@@*=***####*####****#####%%%###%##**#%#******+*###%##%##%%%@%%%##*#######%%%%#%@@@@@@@@@@@@@@@@@@@@@@@@%@%%%%###********************.
.=====+=====+++****%@%@@%*++#####*#*####**##******#####*#%%%#+**%%#**###*****####%%%##@@#%###%**####%%@@@@@%@%@@@@@@@@@@@@@@@@@@@@%%%#%#**#******************.
.===++++++==+*#####@@@@#+*#%##*######*****###**####*###%###%%@@%%%%%%%%%%@@@@%%%##%%@%@%%%%%###%%%#%%%%@@@@@@@@@@@@%@@%%@@@@@%%#%%%@%%#**********************.
.#+======+++*#**#%#%@%#**#######**###%###*#*###################%%%%%%@@@%%%%%#@@%%###%%%%%####%###%%%*#%*+#%%@%%@@@@@%@@@@@@%++************#*****************.
.**#*+*****##%#%%%#@##**#####*#**###**#####*#######%######***#%####%####**##%%%####*#*#########@%%@%%%%@@%*#%@%@@@@@@@@@@@@######*####*###***#***************.
.######***#%##%%%#*%#*############*#####**#*###**#*##%####*##%%##%######%%%@#%#**###%#%##*##%%#%%%@%%%##%@@@@@@@@%%@@%%@%++**#################*#**#******#***.
.##%##*++#%%####%##%#*#%########*###**#####*#****####%%%%%%#****########%#######%%%##***#%%%%%@@%##%%%#***%@%%%%@@@%@@@%@%**++*#********#****########********.
.#%%%%#+*#%%%%%@@%#%%*%############***###%%#######*######%%%@@@%%%#*%##%%######%###*##%@%#@%%%#%#%#%@@@%%@@@%@@@@@@@@@@@@@@@@@@@@@@%@%###*******#########****.
...:::----::::::::::::..:.:.::.::.::.:..:.::::::::::::::::::--:-:---:--:::::::::::-------==-----=====+*#########**#**####%#***+++++++=+====--:::::::..:.......""")
J,Weights = backprop(Weights, X, y, 120, 0.9, 0.1)

Xtest = test_images[:1000,:]
ytest = test_labels[:1000]
ytest = ytest.reshape((ytest.shape[0], 1))
p, acc = ff_predict(Weights, Xtest, ytest)
print('Net accuracy for test set = ', acc)
